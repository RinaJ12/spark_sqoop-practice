sqoop import-all-tables \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username root \
--password cloudera \
--warehouse-dir order_data \
--m 1

Products class
productIterator


secured copy
scp src_folder des_folder

jar tvf file1.jar to see content of jar file in linux

spark-submit 
--class className
--master yarn
--mode cluster/client
classContainingjar.jar prog_arg1 prog_arg2

hadoop fs -du order_data // calculates the file size in folder
1029     1029     order_data/categories
953525   953525   order_data/customers
60       60       order_data/departments
5408880  5408880  order_data/order_items
2999944  2999944  order_data/orders
173993   173993   order_data/products


hadoop fs -du -s order_data // calculate total size
9537431  9537431  order_data

hadoop fs -du -s -h order_data 
9.1 M  9.1 M  order_data

count number of lines in each file in folder (not supported with hadoop fs )
wc -l home/cloudera/categories.java
367 home/cloudera/categories.java

import more than one classes in scala
import org.apache.spark.{SparkConf,SparkContext}

exercises:
1. revenue for each day for completed and closed orders.

join orders(order_id,order_date) & order_items(order_item_subtotal)
1) filter orders by order_status => COMPLETE OR CLOSED
2) join orders and order_items on o.order_id = oi.order_item_order_id
3) group by order_date
4) select order_date,sum(oi.order_item_sub_total)
=======================spark========================
val o = sc.textFile("order_data/orders/").filter(rec=>rec.contains("CLOSED")||rec.contains("COMPLETE")).map(rec=>(rec.split(",")(0).toInt,rec.split(",")(1)))
val oi = sc.textFile("order_data/order_items/").map(rec=>(rec.split(",")(1).toInt,rec.split(",")(4).toDouble))
val o_oi = o.join(oi).map(rec=>(rec._2._1,rec._2._2))
val daily_revenue = o_oi.reduceByKey((acc,value)=>(acc+value)).map(rec=>(rec._1,BigDecimal(rec._2).setScale(2,BigDecimal.RoundingMode.HALF_UP).toDouble).productIterator.mkString("\t"))

->RDD
val daily_revenue = o_oi.reduceByKey((acc,value)=>(acc+value)).map(rec=>(rec._1,BigDecimal(rec._2).setScale(2,BigDecimal.RoundingMode.HALF_UP).toDouble).productIterator.mkString("\t"))
->dataframe

case class Orders(o_id:Int,o_date:String)
case class OrderItems(oi_oid:Int,oi_subtotal:Double)
val o_df = o.map(rec=>Orders(rec._1,rec._2))
val oi_df = oi.map(rec=>OrderItems(rec._1,rec._2))
val o_oi_df = o_df.join(oi_df,"o_id"==="oi_oid")
val daily_revemue = o_oi_df.groupBy("o_date").agg(sum(oi_subtotal))

->spark sql


2. revenue for each day per department for completed and closed orders.
3. How many seats each party would have won in the three way contest
	it is three way contest between BJP,BSP and SP+INC

state - Andhra Pradesh	 												
constituency - Adilabad
candidate_name - GODAM NAGESH
sex - M
age - 49
category - ST 
partyname - TRS
partysymbol - Car
general - 425762
postal - 5085
total - 430847
pct_of_total_votes - 31.07931864
pct_of_polled_votes - 40.81807244
totalvoters - 1386282

===================
Top 5 products for each category
select order_item_product_id,dense_rank() over (partition by order_item_product_id order by order_item_subtotal desc)  as top_5 from order_items


select p.product_id,p.product_name from 
products p join order_items oi on
p.product_id=order_item_product_id

---------------------------------
sqoop import-all-tables \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username root \
--password cloudera \
--hive-import \
--create-hive-table \
--hive-database retail_db_hive \
--m 1

-----------------------------
save hive query output in file
insert overwrite local directory '/home/cloudera/CCA175/hive_query_output/top_5_products'select tbl1.order_item_product_id,top_5 from (select order_item_product_id,dense_rank() over (partition by order_item_product_id order by order_item_subtotal desc)  as top_5 from order_items) tbl1 where top_5<6;
--------------------------------
topN Products(rank) & dense rank

val cat = sc.textFile("order_data/categories").map(rec=>(rec.split(',')(0).toInt,rec.split(',')(2)))

val distinct_price = sc.textFile("order_data/products").filter(rec=>rec.split(',')(4)!="").map(rec=>(rec.split(',')(1).toInt,rec.split(',')(4).toDouble)).groupByKey().map(rec=>(rec._1,rec._2.toList.distinct.sorted.reverse.take(5)))

val prod = sc.textFile("order_data/products").filter(rec=>rec.split(',')(4)!="").map(rec=>(rec.split(',')(1).toInt,rec)).groupByKey().map(rec=>(rec._1,rec._2.toList.sortBy(rec=>rec.split(',')(4).toDouble).reverse)).

val prod_distinct = prod.join(distinct_price).map(rec=>(rec._1,rec._2._1.filter(r=>{rec._2._2.contains(r.split(',')(4).toDouble)}).sortBy(k => -k.split(',')(4).toDouble))) 

val cat_prod =cat.join(prod_distinct).flatMap(rec=>rec._2._2.map(r=>(rec._2._1,r)))
--------------------------------

to use cache an rdd
import org.apache.spark.storage.StorageLevel
rdd.persist(StorageLevel.MEMORY_AND_DISK)

rdd.unpersist()
-----------------------------------
reading and writing sequence File using spark
------------------------------------
all hadoop datatypes are under
org.apache.hadoop.io._ package

saveAsSequenceFile is only available for paried rdd
=>write seq file
rdd.map(rec=>(reckey,recvalue)).saveAsSequenceFile("/path/to/src/file") // it contains the metadata about key value and datatypes of key&val

=>read seq file
sc.sequenceFile("path/to/seq/file",classOf[key_class],classOf[value_class]).map(rec=>rec.toString).collect.foreach(println)

example=>
import org.apache.hadoop.io._
val prod = sc.textFile("order_data/products")
val prod_seq_write = prod.map(rec=>(NullWritable.get(),rec)).saveAsSequenceFile("order_data/seq_files/prod_seq")
val prod_seq_read = sc.sequenceFile("order_date/seq_files/prod_seq",classOf[NullWritable],classOf[Text]).map(rec=>rec.toString)

--------------------------------------------------------------------------
broad cast variable
--------------------------------------------------------------------------

val o = sc.textFile("order_data/orders").map(rec=>{
	val rec_split = rec.split(',')
	(rec_split(0).toInt,rec_split(1))
})

val o_bc = sc.broadcast(o.collectAsMap())
val o_oi_bc = sc.textFile("order_data/order_items").map(rec=>{
val rec_split = rec.split(',')
(o_bc.value.get(rec_split(1).toInt).get,rec_split(4).toDouble)
}) //one stage and no shuffling

val o_oi = o.join(oi)//three stages and shuffling

------------------------------------------------------------------------------
Problem Scenario 30 : You have given following two files 
------------------------------------------------------------------------------
op => Id,name,salary,managerName
id, name
1, Alex Best

val emp_info = sc.textFile("employee/EmployeeName.csv").filter(rec=>{
val temp = rec.split(',')
temp(0)!="id" & temp(1)!="name"
}).map(rec=>{
val temp = rec.split(',')
(temp(0).toInt+","+temp(1)+","+emp_sal_bc.value.get(temp(0).toInt).get+","+emp_mng_bc.value.get(temp(0).toInt).get)
})

emp_info.saveAsTextFile("employee/output/op.txt")

val emp_mng = sc.textFile("employee/EmployeeManager.csv").filter(rec=>{
val temp=rec.split(',')	
temp(0)!="id" & temp(1)!="managerName"
}).map(rec=>{
val temp=rec.split(',')
(temp(0).toInt,temp(1))
})

val emp_mng_bc = sc.broadcast(emp_mng.collectAsMap())


val emp_sal = sc.textFile("employee/Employeesalary.csv").filter(rec=>{
val temp=rec.split(',')
temp(0)!="id" & temp(1)!="salary"
}).map(rec=>{
val temp=rec.split(',')
(temp(0).toInt,temp(1).toDouble)
})

val emp_sal_bc = sc.broadcast(emp_sal.collectAsMap())

------------------------------------------------------------------------------
Problem Scenario 31 : You have given following two files 
------------------------------------------------------------------------------

1. Content.txt : Contain a huge text file containing space separated words. 
2. Remove.txt : Ignore/filter all the words given in this file (Comma Separated). 
Write a Spark program which reads the Content.txt tile and load as an RDD, remove all the words from a broadcast variables (which is loaded as an RDD of words from remove.txt). and count the occurrence of the each word and save ie as a text file  HDFS.

val content = sc.textFile("96_sce/content/content.txt").map(rec=>{
val temp = rec.split(" ").filter(rec=>rec!=null).map(rec=>rec.replaceAll(",|;|:|\\.","").trim())
temp
}).flatMap(rec=>rec)

content: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[106] at flatMap at <console>:30 4195

val remove_text = sc.textFile("96_sce/content/stopwordlist.txt") 429

val final_words = content.subtract(remove_text).map(rec=>(rec,1)).reduceByKey((acc,value)=>acc+value).sortBy(rec=>rec._2)

final_words.saveAsTextFile("96_sce/content/output")

------------------------------------------------------------------------------
Problem Scenario 32 : You have given three files as below. 
------------------------------------------------------------------------------
spark3/sparkdir1/file1 .txt 
spark3/sparkdir2/file2.txt 
spark3/sparkdir3/file3.txt 
Each file contain some text. As given in RHS (Righ hand side). 
Now write a Spark code in scala which will load all these three files from hdfs and do the word count by filtering following words. 
And result should be sorted by word count in reverse order. 
Filter words (“a”,”the”,”an”, “as”, “a”,  “with”, “this”, “this”, “these”, “is”, “are”, “in”, “for”, “to”, “and”, “the”, “of” 
Also please make sure you load all three files as a Single RDD (All three files must be loaded using single API call). 
You have also been given following codec 
import org.apache.hadoop.io.compress.GzipCodec 
Please use above codec to compress tile, while saving in hdfs. 

import org.apache.hadoop.io.compress.GzipCodec 

val filter_words = List("a","the","an","as","a","with","this","this","these","is","are","in","for","to","and","the","of")
val file = sc.wholeTextFiles("96_sce/content/prob_33").map(rec=>rec._2.split("\n")).flatMap(rec=>rec).
filter(rec=>{!(filter_words.contains(rec))}).
map(rec=>(rec,1)).
reduceByKey(_+_).sortBy(rec=> -rec._2).
saveAsTextFile("96_sce/content/prob_33/output",classOf[org.apache.hadoop.io.compress.GzipCodec])

------------------------------------------------------------------------------
Problem Scenario 33 : You have given a files as below. 
------------------------------------------------------------------------------
spark5/EmployeeName.csv (id,name) 
spark5/EmployeeSalary.csv (id,salary) 
Data is given in RHS (Righ hand side). 
Now write a Spark code in scala which will load these two files from hdfs and join the same, and produce the (name,salary) values. 
And save the data in multiple file group by salary (Means each file will have name of employees with same salary). Make sure file name include salary as well.

val emp_info = sc.textFile("employee/EmployeeName.csv").filter(rec=>{
val temp = rec.split(",")
temp(0)!="id" & temp(1)!="name"
}).map(rec=>{
val temp = rec.split(",")
(temp(0).toInt,temp(1))
})

val emp_sal = sc.textFile("employee/Employeesalary.csv").filter(rec=>{
val temp=rec.split(",")
temp(0)!="id" & temp(1)!="salary"
}).map(rec=>{
val temp=rec.split(",")
(temp(0).toInt,temp(1).toDouble)
})

val emp_info_sal = emp_info.join(emp_sal).map(rec=>(rec._2._1,rec._2._2))
val emp_group = emp_info_sal.map(rec=>(rec._2,(rec._1,rec._2))).groupByKey().collect()

emp_group: Array[(Double, Iterable[(String, Double)])] = Array((200.0,CompactBuffer(( Bill Clinton,200.0), ( pqr,200.0))), (300.0,CompactBuffer(( stu,300.0), ( Cayman Dayton,300.0))), (100.0,CompactBuffer(( abc,100.0), ( Alex Best,100.0))), (400.0,CompactBuffer(( Don Eastern,400.0), ( def,400.0))), (500.0,CompactBuffer(( mno,500.0), ( Eve Flex,500.0))))


val arr = Array((200.0,List(( "Bill Clinton",200.0), ("pqr",200.0))), (300.0,List(( "stu",300.0), ( "Cayman Dayton",300.0))), (100.0,List(( "abc",100.0), ( "Alex Best",100.0))), (400.0,List(( "Don Eastern",400.0), ( "def",400.0))), (500.0,List(( "mno",500.0), ( "Eve Flex",500.0))))

val prob_33 = emp_group.map{case(k,v)=>k->sc.makeRDD(v.toSeq)}
prob_33.foreach{case(k,rdd)=>rdd.saveAsTextFile("employee/output/"+k)}

-------------------------------------------------------------------------------------
Problem Scenario 34 : You have given a tile named spark6/user.csv. 
-------------------------------------------------------------------------------------
Data is given in RHS (Righ hand side). 
Now write a Spark code in scala which will remove the header part and create ROD of values as below, for all rows. And also if id is "myself" than filter out row.

Map(id 0m, topic scala, hits 120) 

val header = sc.textFile("96_sce/topics/topics.csv").first.split(",").toList

val topics = sc.textFile("96_sce/topics/topics.csv").filter(rec=>rec.split(",")(0)!=header(0) & rec.split(",")(1)!=header(1) & rec.split(",")(2)!=header(2) & rec.split(",")(1)!="myself").map(rec=>{
val temp=rec.split(",")
(header(0)+" "+temp(0)+","+header(1)+" "+temp(1)+","+header(2)+" "+temp(2))
})

								OR
val topics = sc.textFile("96_sce/topics/topics.csv").filter(rec=>rec.split(",")(0)!=header(0) & rec.split(",")(1)!=header(1) & rec.split(",")(2)!=header(2) & rec.split(",")(1)!="myself").
map(rec=>rec.split(",")).
map(rec=>header.zip(rec)).
map(rec=>rec(0)._1+" "+rec(0)._2+","+rec(1)._1+" "+rec(1)._2+","+rec(2)._1+" "+rec(2)._2

val topics = sc.textFile("96_sce/topics/topics.csv").filter(rec=>rec.split(",")(0)!=header(0) & rec.split(",")(1)!=header(1) & rec.split(",")(2)!=header(2) & rec.split(",")(1)!="myself").
map(rec=>header.zip(rec.split(","))).
map(rec=>rec(0)._1+" "+rec(0)._2+","+rec(1)._1+" "+rec(1)._2+","+rec(2)._1+" "+rec(2)._2)


topics.saveAsTextFile("96_sce/topics/output")

-----------------------------------------------------------------------------------------
Problem Scenario 35 : You have been given a file named spark7/EmployeeName.csv (id,name).
----------------------------------------------------------------------------------------- 
1. Load this file from hdfs and sort it by name and save it back as (id,name) in results directory. However, make sure while saving it should be able to write in a single file.

val emp_info = sc.textFile("employee/EmployeeName.csv").map(rec=>{
val temp = rec.split(",")
(temp(0),temp(1))
}).sortBy(rec=>rec._2,true,1) // data will be saved in one partition or use .repartition(1)

emp_info.saveAsTextFile("employee/output/sortByName")

-----------------------------------------------------------------------------------------
Problem Scenario 36 : You have been given a tile named spark8/data.csv (type,name). 
-----------------------------------------------------------------------------------------
1. Load this file from hdfs and save it back as (id, (all names of same type)) in results directory. However, make sure while saving it should be able to write in a single file.

val names = sc.textFile("96_sce/name_type/name_type.csv").map(rec=>{
val temp = rec.split(",")
(temp(0),temp(1))
}).groupByKey().map(rec=>(rec._1,rec._2.toList)).repartition(1).saveAsTextFile("96_sce/name_type/output")

-----------------------------------------------------------------------------------------
Problem Scenario 37 : HadoopExam.com has done survey on their Exam Products feedback using a web based form.
With the following tree text field as input in web ui.
-----------------------------------------------------------------------------------------
Name : String 
Subscription Date : String 
Rating : String 
And servey data has been saved in a tile called spark9/feedback.txt 
Christopher|Jan 11, 2015|5 
Kapil|11 jan, 2015|5
Thomas|6/17/2014|5 
J0hn|22-08-2013|5 
Mithun|2013|5 
Jitendra||5 
Write a spark program using regular expression which will filter all the valid dates and save in two separate tile (good record and bad record)

val regex1 = """(\d{1,2})\s(\w{3})(,)\s(\d{4})""".r //"11 Jan,2015"
val regex2 = """(\w{3})\s(\d{1,2})(,)\s(\d{4})""".r //jan 11, 2015
val regex3 = """(\w{1,2})\/(\w{1,2})\/(\w{4})""".r //6/17/2014
val regex4 = """(\d{1,2})(-)(\d{1,2})(-)(\d{4})""".r //22-08-2013

    println("11 Jan,2015".matches(regex1.toString()))
    println("jan 11, 2015".matches(regex2.toString()))
    println("06/17/2014".matches(regex3.toString()))
    println("22-08-2013".matches(regex4.toString()))

val survey_good = sc.textFile("96_sce/survey/survey.txt").map(rec=>rec.split('|')).filter(rec=>{
rec(1).matches(regex1.toString) | rec(1).matches(regex2.toString) | rec(1).matches(regex3.toString) | rec(1).matches(regex4.toString)
}).map(rec=>(rec(0),rec(1),rec(2)))

survey_good.saveAsTextFile("96_sce/survey/good")

val survey_bad = sc.textFile("96_sce/survey/survey.txt").map(rec=>rec.split('|')).filter(rec=>{
!rec(1).matches(regex1.toString) & !rec(1).matches(regex2.toString) & !rec(1).matches(regex3.toString) & !rec(1).matches(regex4.toString)
}).map(rec=>(rec(0),rec(1),rec(2)))

survey_bad.saveAsTextFile("96_sce/survey/bad")


or

 val survey = sc.textFile("96_sce/survey/survey.txt").map(rec=>rec.split('|'))
 survey: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[662] at map at <console>:33

val new_try = survey.groupBy(rec=>{
if(rec(1).matches(regex1.toString) | rec(1).matches(regex2.toString) | rec(1).matches(regex3.toString) | rec(1).matches(regex4.toString))
"good"
else
"bad"
}).map(rec=>(rec._1,rec._2.map(r=>r.mkString("|")))).collect

new_try.foreach{case(k,v)=>
(sc.parallelize(v.toSeq)).saveAsTextFile("96_sce/survey/"+k+"1")
}

------------------------------------------------------------------------
Problem Scenario 38 : You have been given an RDD as below. 
------------------------------------------------------------------------
=>val rdd: RDD[array[Byte]]
=>Now you have to save this RDD as a SequenceFile. And below is the code snippet. 
 as we have to save above rdd as sequence file so conevrt into pairedRDD
import org.apache.hadoop.io.compress.GzipCodec 
rdd.map(bytesArray => (A.get(), new B(bytesArray))).saveAsSequenceFile ("/output/path",classOt[GzipCodec]) 

rdd.map(rec=>(NullWritable.get(),new BytesWritable(rec))).saveAsSequenceFile("/output/path",classOf[GzipCodec])
What would be the correct replacement tor A and B in above snippet. 

------------------------------------------------------------------------
Problem Scenario 39 : You have been given two files 
------------------------------------------------------------------------
spark16/file1 .txt 
spark16/file2.txt 
1,9,5
2,7,4
3,8,3
Spark16/file2.txt
1,g,h
2,I,j
3,k,l
Load these two files as Spark RDD and join them to produce the below results 
(1, ( (9,5), (g,h) )) 
(2, ( (7,4), (i,j) )) 
(3, ( (8,3), (k,l) )) 
And write code snippet which will sum the second columns of above joined results (5+4+3). 


val f1 = sc.textFile("96_sce/prob_39/file1.txt").map(rec=>{
val temp =rec.split(",")
(temp(0).toInt,(temp(1).toInt,temp(2).toInt))
})

or 
val f1 =sc.textFile("96_sce/prob_39/file1.txt").map(rec=>{
rec.split(",") match {case Array(k,v,p)=>(k.toInt,(v.toInt,p.toInt))}
})


val f2 = sc.textFile("96_sce/prob_39/file2.txt").map(rec=>{
val temp =rec.split(",")
(temp(0).toInt,(temp(1),temp(2)))
})

or

val f2 = sc.textFile("96_sce/prob_39/file2.txt").map(rec=>{
rec.split(",") match {case Array(k,v,p)=>(k.toInt,(v,p))}
})

val f1_f2 = f1.join(f2)
f1_f2: org.apache.spark.rdd.RDD[(Int, ((Int, Int), (String, String)))] = MapPartitionsRDD[202] at join at <console>:31

val l = f1_f2.map(rec=>rec._2._1._2).sum

or

val l =f1_f2.map(rec=>{
rec match {case ((_,((_,num1),(_,_))))=>num1}
}).reduce(_+_)

------------------------------------------------------------------------------------------------------------
Problem Scenario 40 : You have been given sample data as below in a file called spark15/tile1 .txt 
------------------------------------------------------------------------------------------------------------
3070811,1963,1096,, “US”, “CA”,,1,
3070811,1963,1096,, “US”, “CA”,,1,56
3070811,1963,1096,, “US”, “CA”,,1,23
Below is the code snippet to process this file. 
val field=sc.textFile("96_sce/prob_40/prob_40.txt")
val mapper = field.map(x=> x.split(",")) 
mapper.map(x => x.map(x=> {if(x == "") 0 else x})).collect
Please fill in A and B so it can generate below final output 
Array(Array(3070811, 1963,1096, 0, “US”, “CA”,,1, 0)
,Array(3022811, 1963,1096, 0, “US”, “CA”,,1, 56)
,Array(3033811, 1963,1096, 0, “US”, “CA”,,1, 23)

------------------------------------------------------------------------------------------------------------
Problem Scenario 41 : You have been given below code snippet. 
------------------------------------------------------------------------------------------------------------
val au1 = sc.parallelize(List ( ("a" , Array(1 ,2)) , ("b" , Array(1 ,2)))) 
val au2 = sc.parallelize(List ( ("a" , Array(3)) , ("b" , Array(2)))) 
Apply the Spark method, which will generate below output. 
Array[(String, Array[lnt])] = Array((a,Array(1, 2)), (b,Array(1, 2)), (a,Array(3)), (b,Array(2)))

au1.union(au2)

------------------------------------------------------------------------------------------------------------
Problem Scenario 42 : You have been given a file (spark10/sales.txt), with the content as given in RHS. 
------------------------------------------------------------------------------------------------------------
And want to produce the output as a csv with group by Department,Designation,State with additional columns with sum(costToCompany) and TotalEmployeeCount

Should get result like 

Dept,Desg,state,empCount,totalCost 
Sales,Lead,AP,2,64000
Sales,Lead,LA,3,96000
Sales,Lead,TN,2,64000

case class Emp(Dept:String,Desg:String,state:String,empCount:Int,totalCost:Long)
val emp = sc.textFile("96_sce/prob_42/prob_42.txt").filter(rec=>{
val temp=rec.split(",")
temp(0)!="Dept" & temp(1)!="Desg" & temp(2)!="state" & temp(3)!="empCount" & temp(4)!="totalCost"
}).map(rec=>{
val temp=rec.split(",")
Emp(temp(0),temp(1),temp(2),temp(3).toInt,temp(4).toLong)
}).

val gbk_emp = emp.map(em=>((em.Dept,em.Desg,em.state),(em.empCount,em.totalCost))).
reduceByKey((rec1,rec2)=>(rec1._1+rec2._1,rec1._2+rec2._2))

-----------------------------------------------------------------------------------------------------------
Problem Scenario 43 : You have been given following code snippet. 
-----------------------------------------------------------------------------------------------------------
val grouped = sc.parallelize(Seq(((1,"two"), List((3,4), (5,6))))) 
val flattened = grouped.flatMap { case (rec,groupValues)=> 
groupValues.map { value => (rec._1,rec._2,value._1,value._2) } 
You need to generate following output. Hence replace A and B 
Array((1,two,3,4),(1,two,5,6))

org.apache.spark.rdd.RDD[((Int, String), List[(Int, Int)])]

//my solution
val a =grouped.map(rec=>{
(rec._2.map(r=>(rec._1._1,rec._1._2,r._1,r._2)))
}).flatMap(rec=>rec).collect

-----------------------------------------------------------------------------------------------------------
Problem Scenario 44 : You have been given 4 files , with the content as given in RHS. 
-----------------------------------------------------------------------------------------------------------

(spark11/file1 .txt) 
(spark11/file2.txt) 
(spark11/file3.txt) 
(spark11/file4.txt) 
Write a Spark program, which will give you the highest occuring words in each file. With their file name and highest occuring words. 

val f1 = sc.textFile("96_sce/prob_44/file1.txt").map(rec=>(rec,1)).reduceByKey(_+_).map(rec=>rec.swap).max
val f2 = sc.textFile("96_sce/prob_44/file2.txt").map(rec=>(rec,1)).reduceByKey(_+_).map(rec=>rec.swap).max
val f3 = sc.textFile("96_sce/prob_44/file3.txt").map(rec=>(rec,1)).reduceByKey(_+_).map(rec=>rec.swap).max
val f4 = sc.textFile("96_sce/prob_44/file4.txt").map(rec=>(rec,1)).reduceByKey(_+_).map(rec=>rec.swap).max

val files = sc.wholeTextFiles("96_sce/prob_44").map(rec=>(rec._1.split('/')(7),rec._2.split("\n").map(r=>(r,1)))).map(rec=>(rec._1,rec._2.groupBy(rec=>rec._1).map{r=>r._2}.sum)).toList.map(rec=>rec.swap).max

val files = sc.wholeTextFiles("96_sce/prob_44").
map(rec=>(rec._1.split('/')(7),rec._2.split("\n").map(r=>(r,1)))).
map(rec=>(rec._1,rec._2.groupBy(rec=>rec._1).map(rec=>(rec._1,rec._2.map{r=>r._2}.sum)).toList.map(rec=>rec.swap).max)).map(rec=>rec._1+"\t"+rec._2._1+"\t"+rec._2._1)

files.saveAsTextFile("96_sce/prob_44_output")

-----------------------------------------------------------------------------------------------------------
Problem Scenario 45 : You have been given 2 files , with the content as given in RHS. 
-----------------------------------------------------------------------------------------------------------
(spark12/technology.txt) 

joe,doe,hadoop
jane,doe,spark
john,smith,hortonworks
jennifer,smith,cloudera

(spark12/salary.txt) 
joe,doe,100
jane,doe,200
john,smith,300
jennifer,smith,400

Write a Spark program, which will join the data based on first and last name and save the joined results in following format. 

first,last,technology,salary 
==================================================================================
case class Tech(fname:String,lname:String,tech:String)
case class Salary(fname:String,lname:String,salary:Long)

val tech = sc.textFile("96_sce/prob_45/tech.txt").map(rec=>{
val temp= rec.split(",")
Tech(temp(0),temp(1),temp(2))
}).map(rec=>((rec.fname,rec.lname),rec.tech))

val salary = sc.textFile("96_sce/prob_45/salary.txt").map(rec=>{
val temp= rec.split(",")
Salary(temp(0),temp(1),temp(2).toLong)
}).map(rec=>((rec.fname,rec.lname),rec.salary))

val tech_sal = tech.join(salary).map(rec=>rec._1._1+","+rec._1._2+","+rec._2._1+","+rec._2._2)

tech_sal.saveAsTextFile("96_sce/prob_45/output")

-----------------------------------------------------------------------------------------------------------
Problem Scenario 46 : You have been given belwo list in scala (name,sex,cost) for each work done. 
-----------------------------------------------------------------------------------------------------------
List( ("Deepak" , "male", 4000), ("Deepak" , "male", 2000), ("Deepika" , "female", 2000),("Deepak" , "female", 2000), ("Deepak" , "male", 1000) 
Now write a Spark program to load this list as an RDD and do the sum of cost for combination of name and sex (as key)

val list_rdd = sc.parallelize(List(("Deepak" , "male", 4000), ("Deepak" , "male", 2000), ("Deepika" , "female", 2000),("Deepak" , "female", 2000), ("Deepak" , "male", 1000)))

or 
val list_rdd = sc.makeRDD(List(("Deepak" , "male", 4000), ("Deepak" , "male", 2000), ("Deepika" , "female", 2000),("Deepak" , "female", 2000), ("Deepak" , "male", 1000))).map(rec=>((rec._1,rec._2),rec._3)).reduceByKey(_+_)

-----------------------------------------------------------------------------------------------------------
Problem Scenario 47 : You have been given below code snippet, with intermediate output. 
-----------------------------------------------------------------------------------------------------------

val z = sc.parallelize(List(1,2,3,4,5,6), 2) 

// lets first print out the contents ot the RDD with partition labels 

def myfunc(index: Int, iter:Iterator[Int]) : Iterator[String] = { 
iter.toList.map(x => "[partID:" + index + ", val: " + x + "]").iterator} 

ln each run , output could be different, while solving problem assume below output only:

scala> z.mapPartitionsWithIndex(myfunc).collect
res24: Array[String] = Array([partID:0, val: 1], [partID:0, val: 2], [partID:0, val: 3], [partID:1, val: 4], [partID:1, val: 5], [partID:1, val: 6])

Now apply aggreate method on RDD z , with two reduce function , first will select max value in each partition and second will add all the maximum values from all parttions
Initialize the aggregate with value 5, hence expected output will be 16. 

//mapPArtitions only return Iterator[]
//prob_47: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[429] at mapPartitionsWithIndex at <console>:29

//def
//aggregate[B](z: ⇒ B)(seqop: (B, T) ⇒ B, combop: (B, B) ⇒ B): B

val prob_47 = z.mapPartitionsWithIndex((index,rec)=>rec.map{r=>(index,r)})
res109: Array[(Int, Int)] = Array((0,1), (0,2), (0,3), (1,4), (1,5), (1,6))

prob_47.aggregate(5)((acc:Int,rec2:(Int,Int))=>math.max(acc,rec2._2),(acc:Int,rec2:Int)=>acc+rec2) //Initialize the aggregate with value 5, hence expected output will be 16.
//As acc is initialized with 5 so it is going to be 5+(max in paritition 0)5+(max in partition 1) 6 = 16

-----------------------------------------------------------------------------------------------------------
Problem Scenario 48 : You have been given below Python code snippet, with intermediate output. 
-----------------------------------------------------------------------------------------------------------
We want to take a list ot records about people and then we want to sum up their ages and count them. 
So tor this example the type in the RDD will be a Dictionary in the format of {name: NAME, age:AGE, gender:GENDER}. 

The result type will be a tuple that looks like so (Sum ot Ages, Count) 

people = O 
people.append({'name':'Amit', 'age':45,'gender':'M'}) 
people.append({'name':'Ganga', 'age':43,'gender':'F'}) 
people.append({'name':'John', 'age':28,'gender':'M'}) 
people.append({'name':'Lolita', 'age':33,'gender':'F'}) 
people.append({'name':'Dont Know', 'age':18,'gender':'T'}) 
peopleRdd=sc.parallelize(people) //Create an RDD 
peopleRdd.aggregate((0,0), seqOp, combOp) 
//Output ot above line : 167, 5) 
Now define two operation seqOp and combOp , such that 
seqOp : Sum the age ot all people as well count them, in each partition. 
combOp : Combine results from all partitions. 

seq : (acc:(Int:Int),data:(String,Int,String))=>(acc._1+data._2,acc._2+1)
combOp : ((rec1:(Int,Int),rec2:(Int,Int))))=>(rec1._1+rec2._1,rec._1._2+rec._2._2))


-----------------------------------------------------------------------------------------------------------
Problem Scenario 49 : You have been given below code snippet (do a sum of values by key), with intermediate output. 
-----------------------------------------------------------------------------------------------------------
val keysWithValuesList = Array("foo=A", "foo=A", "foo=A", "foo=A", "foo=B", "bar=C", "bar=D", "bar=D") 
val data = sc.parallelize(keysWithValuesList) 
//Create key value pairs 
val kv = data.map(_.split("=")).map(v => (v(0), v(1))).cache() 
val initialCount = 0; 
val countByKey = kv.aggregateByKey(initialCount)(((acc:Int,data:String)=>acc+1), ((acc:Int,data:Int)=>acc+data)) 
Now define two functions (addToCounts, sumPartitionCounts) such, which will produce following results. 
Output 1 
countByKey.collect 
res3: Array[(String, Int)] = Array((foo,5), (bar,3)) 

solutions
addToCounts ((acc:Int,data:String)=>acc+1)
sumPartitionCounts((acc:Int,data:Int)=>acc+data)


import scala.collection. _
val initialSet = scala.collection.mutable.HashSet.empty[String] 
val uniqueByKey = kv.aggregateByKey(initialSet)(((acc:scala.collection.mutable.HashSet[String],data:String)=>acc+=data), ((acc:scala.collection.mutable.HashSet[String],data:Set[String])=>acc++=data)) 

Now define two functions (((acc:Set[String],data:String)=>acc+data), ((acc:Set[String],data:Set[String])=>acc++data) such, which will produce following results. 
Output 2 : 
uniqueByKey.collect 
res4: Array[(String, scala.collection.mutable.HashSet[String])] = Array(( foo,set(B,A)),(bar,Set(C,D)))

-----------------------------------------------------------------------------------------------------------
Problem Scenario 50 : You have been given below code snippet (calculating an average score), with intermediate output. 
-----------------------------------------------------------------------------------------------------------

type Scorecollector= (Int, Double) 
type Personscores = (String, (Int, Double)) 
val initialscores = Array((“Fred”, 88.0), ("Fred", 95.0), ("Fred", 91.0), ("Wilma", 93.0), ("Wilma", 95.0), ("Wilma", 98.0)) 
val wilmaAndFredScores = sc.parallelize(initialScores).cache()

val scores = wilmaAndFredScores.combineByKey(((rec:Double)=>(1,rec)),((acc:(Int,Double),rec:Double)=>(acc._1+1,acc._2+rec)), ((acc:(Int,Double),rec:(Int,Double))=>(acc._1+rec._1,acc._2+rec._2))) 



val averagingFunction = (personscore: Personscores) => { 
val (name, (numberscores, totalScore)) = personscore 
(name, totalScore / numberscores) }
val averagescores = scores.collectAsMap().map(averagingFunction) 
Expected output : averagescores: scala.conection.Map[string,Double] = Map(Fred-> 91.33333333333333, Wilma-> 95.33333333333333) 
Define all three required function , which are input tor combineByKey method. e.g. (createScoreCombiner, scorecombiner, scoreMerger).And help us producing required results 
==================================================================================

-----------------------------------------------------------------------------------------------------------
Problem Scenario 51 : You have been given below code snippet. 
-----------------------------------------------------------------------------------------------------------
val a = sc.parallelize(List(1, 2, 1, 3), 1) 
val b = a.map((_, "b")) 
val c = a.map((_, "c")) 
Operation_xyz 
val d = b.cogroup(c)

Write a correct code snippet for Operation_xyz which will produce below output. 

Output : 

Array[(lnt, (Iterable[String], Iterable[String]))] = Array( 
(2,(ArrayBuffer(b),ArrayBuffer(c))), 
(3,(ArrayBuffer(b),ArrayBuffer(c))), 
(1,(ArrayBuffer(b, b),ArrayBuffer(c, c))) 

-----------------------------------------------------------------------------------------------------------
Problem Scenario 52 : You have been given below code snippet. 
-----------------------------------------------------------------------------------------------------------
val anRDD = sc.parallelize(List(1,2,3,4,5,6,7,8,2,4,2,1,1,1,1,1))
Operation_xyz 
val gr = anRDD.groupBy(rec=>rec).map(rec=>(rec._1,rec._2.count)).collectAsMap
Array[(Int, Iterable[Int])]

or 

 val gr = anRDD.countByValue()
gr: scala.collection.Map[Int,Long] = Map(5 -> 1, 1 -> 6, 6 -> 1, 2 -> 3, 7 -> 1, 3 -> 1, 8 -> 1, 4 -> 2)

Write a correct code snippet for Operation _xyz which will produce below output
Scala.collection.Map[Int,Long]= Map(5->1,8->1,3->1,6->1,1->6,2->3,4->2,7->1) 
-----------------------------------------------------------------------------------------------------------
problem Scenario 53 : You have been given below code snippet. 
-----------------------------------------------------------------------------------------------------------

val a = sc.parallelize(1 to 10, 3) 
b=a.filter(rec=>rec%2==0)
b.collect 
Output 1 
Array[lnt] = Array(2,4,6,8,10) 
val c=a.filter(rec=>rec<4) 
Output 2 
Array[lnt] = Array(1,2,3) 
Write a correct code snippet tor operation1 and operation2 which will produce desired output, shown above.

-----------------------------------------------------------------------------------------------------------
Problem Scenario 54 : You have been given below code snippet. 
-----------------------------------------------------------------------------------------------------------
val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "panther", "eagle")) 
val b = a.map(x => (x.length, x)) 

operation 1 
res36: Array[(Int, String)] = Array((3,dog), (5,tiger), (4,lion), (3,cat), (7,panther), (5,eagle))

 b.reduceByKey(_+_).sortByKey().collect
Write a correct code snippet for operation1 which will produce desired output, shown below. 

Array[(Int,String)] = Array((4,lion), (7,panther), (3,dogcat), (5,tigereagle)) 

-----------------------------------------------------------------------------------------------------------
Problem Scenario 55 : You have been given below code snippet. 
-----------------------------------------------------------------------------------------------------------
val pairRDD1 = sc.parallelize(List( ("cat",2), ("cat", 5), ("book", 4),("cat", 12))) 
val pairRDD2 = sc.parallelize(List( ("cat" ,2), ("cup", 5), ("mouse", 4),("cat", 12))) 
operation 1 
Write a correct code snippet for operation1 which will produce desired output, shown below. 
Array[(String, (Option[lnt], Option[lnt]))] = Array((book,(Some(4),None)), (mouse,(None,Some(4))), (cup,(None,Some(5))), (cat,(Some(2),Some(2))), (cat,(Some(2),Some(12))), (cat,(Some(5),Some(2))), (cat,(Some(5),Some(12))), (cat,(Some(112),Some(2))), (cat,(Some(12),Some(12))),


-----------------------------------------------------------------------------------------------------------
Problem Scenario 56 : You have been given below code snippet. 
-----------------------------------------------------------------------------------------------------------

val a = sc.parallelize(1 to 100, 3) 
operation 1 

a.glom().collect() // a.glom collect all the elements of a partition into an array

Write a correct code snippet for operation1 which will produce desired output, shown below. 

Array[Array[Int]] = Array(Array(1,2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,26,27,28,29,30,31,32,33), 
Array(34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 6O, 61, 62, 63, 64, 65, 66), 
Array(67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 
94, 95, 96,97, 98, 99, 100)) 

-----------------------------------------------------------------------------------------------------------
Problem Scenario 57 : You have been given below code snippet. 
-----------------------------------------------------------------------------------------------------------
val a = sc.parallelize(1 to 9, 3) 


val result = a.map(rec=>{
if(rec%2==0) {("even",rec)}
else {("odd",rec)}
}).groupByKey()

operation 1 

result = a.groupBy(rec=>if(rec%2==0 "even" else "odd").collect

Write a correct code snippet tor operation1 which will produce desired output, shown below. 
Array[(String, Seq[lnt])] = Array((even,ArrayBuffer(2, 4, 6, 8)), (odd,ArrayBuffer(1, 3, 5, 7, 9))) 

-----------------------------------------------------------------------------------------------------------
Problem Scenario 58 : You have been given below code snippet. 
-----------------------------------------------------------------------------------------------------------
val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "spider", "eagle"), 2) 
val b = a.keyBy(_.length) 

operation 1 => b.groupByKey().collect

Write a correct code snippet for operation1 which will produce desired output, shown below. 
Array[(lnt, Seq[String])] = Array((4,ArrayBuffer(lion)), (6,ArrayBuffer(spider)), (3,ArrayBuffer(dog, cat)), (5,ArrayBuffer(tiger, eagle))) 

-----------------------------------------------------------------------------------------------------------
Problem Scenario 59 : You have been given below code snippet. 
-----------------------------------------------------------------------------------------------------------
val x = sc.parallelize(1 to 20) 
val y = sc.parallelize(10 to 30) 

operation 1 => val z = x.intersection(y) 

z.collect 

Write a correct code snippet tor operation1 which will produce desired output, shown below. 

Array[Int] = Array(16, 12, 20, 13, 17, 14, 18, 10, 19, 15, 11) //common elements in both lists

-----------------------------------------------------------------------------------------------------------
problem Scenario 60 : You have been given below code snippet. 
-----------------------------------------------------------------------------------------------------------

val a = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"), 3) 
val b = a.keyBy(_.length) 
b.collect
res9: Array[(Int, String)] = Array((3,dog), (6,salmon), (6,salmon), (3,rat), (8,elephant))

val c =sc.parallelize(List("dog", "cat", "gnu", "salmon", "rabbit", "turkey", "wolf", "bear", "bee"),3)
val d = c.keyBy(_.length)

d.collect
res8: Array[(Int, String)] = Array((3,dog), (3,cat), (3,gnu), (6,salmon), (6,rabbit), (6,turkey), (4,wolf), (4,bear), (3,bee))


operation1 = > val e = b.join(d) 

Write a correct code snippet tor operation1 which will produce desired output, shown below.

Array[(Int,(String, String))] = Array((6,(salmon,salmon)), (6,(salmon,rabbit)), (6,(salmon,turkey)), (6,(salmon,salmon)), (6,(salmon,rabbit)), (6,(salmon,turkey)), (3,(dog,dog)), (3,(dog,cat)), (3,(dog,gnu)), (3,(dog,bee)), (3,(rat,dog)), (3,(rat,cat)), (3,(rat,gnu)), (3,(rat,bee)))

-----------------------------------------------------------------------------------------------------------
Problem Scenario 61 : You have been given below code snippet. 
-----------------------------------------------------------------------------------------------------------

val a = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"), 3) 
val b = a.keyBy(_.length) 
res3: Array[(Int, String)] = Array((3,dog), (6,salmon), (6,salmon), (3,rat), (8,elephant))

val c = sc.parallelize(List("dog", "cat", "gnu", "salmon", "rabbit", "turkey", "wolf", "bear", "bee"), 3) 
val d = c.keyBy(_.length) 
res4: Array[(Int, String)] = Array((3,dog), (3,cat), (3,gnu), (6,salmon), (6,rabbit), (6,turkey), (4,wolf), (4,bear), (3,bee))

operation 1 => val e = b.rightOuterJoin(d)

Write a correct code snippet tor operation1 which will produce desired output, shown below. 
Array[(Int,(String, Option[String]))] = Array((6,(salmon,Some(salmon))), (6,(salmon,Some(rabbit))), (6,(salmon,Some(turkey))), (6,(salmon,Some(salmon))),  
(6,(salmon,Some(rabbit))), (S,(salmon,some(turkey))), (3,(dog,Some(dog))), (3,(dog,Some(cat))), (3,(dog,Some(gnu))), (3,(dog,Some(bee))), (3,(rat,Some(dog))),
(3,(rat,Some(cat))), (3,(rat,Some(gnu))), (3,(rat,Some(bee))), (8,(elephant,None)))

-----------------------------------------------------------------------------------------------------------
Problem Scenario 62 : You have been given below code snippet. 
-----------------------------------------------------------------------------------------------------------
val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "panther", "eagle"), 2) 
val b = a.map(x => (x.length, x)) 
operationl
b.map(rec=>(rec._1,"x"+rec._2))
or
b.mapValues(rec=>"x"+rec).collect
Write a correct code snippet for operation1 which will produce desired output, shown below. 
Array[(Int,String)] = Array((3,xdogx), (5,xtigerx), (4,xlionx), (3,xcatx), (7,xpantherx), (5,xeaglex))  

-----------------------------------------------------------------------------------------------------------
Problem Scenario 63 : You have been given below code snippet. 
-----------------------------------------------------------------------------------------------------------

val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "panther", "eagle"), 2) 
val b = a.map(x => (x.length, x)) 
operation1

Write a correct code snippet tor operation1 which will produce desired output, shown below. 
Array[(Int,String)] = Array((4,lion), (3,dogcat), (7,panther), (5,tigereagle))

-----------------------------------------------------------------------------------------------------------
Problem Scenario 64 : You have been given below code snippet. 
-----------------------------------------------------------------------------------------------------------

val a = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"), 3) 
val b = a.keyBy(_.length) 
val c = sc.parallelize(List("dog", "cat", "gnu", "salmon", "rabbit", "turkey", "wolf", "bear", "bee"), 3) 
val d = c.keyBy(_.length) 

b.rightOuterJoin(d).collect

operation 1 
Write a correct code snippet tor operation1 which will produce desired output, shown below. 
Array[(Int,(Option[String], String))] = Array((6,(Some(salmon),salmon)), (6,(Some(salmon),rabbit)), (6,(Some(salmon),turkey)), (6,(Some(salmon),salmon)),
(6,(Some(salmon),rabbit)), (6,(Some(salmon),turkey)), (3,(Some(dog),dog)), (3,(Some(dog),cat)), (3,(Some(dog),gnu)), (3,(Some(dog),bee)), (3,(Some(rat),dog)) 
(3,(Some(rat),cat)), (3,(Some(rat),gnu)), (3,(Some(rat),bee)), (4,(None,wolt)), (4,(None,bear)))

-----------------------------------------------------------------------------------------------------------
Problem Scenario 65 : You have been given below code snippet. 
-----------------------------------------------------------------------------------------------------------

val a = sc.parallelize(List("dog","cat", "owl", "gnu", "ant"), 2) 
val b = sc.parallelize(1 to a.count.tolnt, 2) 

val c = a.zip(b)

res0: Array[(String, Int)] = Array((dog,1), (cat,2), (owl,3), (gnu,4), (ant,5))

operation 1 =>
c.sortByKey(false).collect

Write a correct code snippet tor operation1 which will produce desired output, shown below. 
Array[(String, Int)] = Array((owl,3), (gnu,4), (dog,l), (cat,2), (ant,5)) 

-----------------------------------------------------------------------------------------------------------
Problem Scenario 66 : You have been given below code snippet. 
-----------------------------------------------------------------------------------------------------------

val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "spider", "eagle"), 2) 
val b = a.keyBy(_.length) 

res4: Array[(Int, String)] = Array((3,dog), (5,tiger), (4,lion), (3,cat), (6,spider), (5,eagle))

val c = sc.parallelize(List("ant", "falcon", "squid"), 2)  
val d = c.keyBy(_.length) 

res5: Array[(Int, String)] = Array((3,ant), (6,falcon), (5,squid))

operation 1  b.subtractByKey(d) // check PairRDDFucntion in spark scala API for Paired RDD

Write a correct code snippet for operationl which will produce desired output, shown below. 
Array((4,lion))
-----------------------------------------------------------------------------------------------------------
Problem Scenario 67 : You have been given below code snippet. 
-----------------------------------------------------------------------------------------------------------

lines = sc.parallelize(List['its fun to have fun,','but you have to know how.']) 
rl = lines.map( lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower()) 
r2 = rl.flatMap(lambda x: x.split()) 
r3 = r2.map(lambda x: (x, 1)) 
operationl r4 = r3.reduceByKey(lambda x,y :x+y)
r5 = r4.map(lambda x:(x[1].x[0])) 
r6 = r5.sortByKey(ascending=False) 
r6.take(20) 
Write a correct code snippet for operationl which will produce desired output, shown below. 
[(2,'fun'), (2, 'to'), (2, 'have'), (1, 'its'), (1, 'know'), (1, 'how'), (1,'you'), (1, 'but')) 

Scala

val list = sc.parallelize(List("its fun to have fun,","but you have to know how."))
val formatted = list.map(rec=>(rec.replaceAll(",|\\.|-","").toLowerCase())).
flatMap(rec=>rec.split(" ")).map(rec=>(rec,1)).
reduceByKey(_+_).
map(rec=>rec.swap).
sortByKey(false)
==========================================================================
al a = sc.parallelize(List("cat","horse",4.0,3.5,2,"dog"))
val b = a.flatMap {
     |   case v: Int => Some(v)
     |   case v: Double => Some(v)
     |   case v:String => Some("itsstring")
     | }
b: org.apache.spark.rdd.RDD[Any] = MapPartitionsRDD[293] at flatMap at <console>:32

scala> b.collect
res116: Array[Any] = Array(itsstring, itsstring, 4.0, 3.5, 2, itsstring)

==========================================================================
Problem Scenario 68 : You have given a file as below. 
==========================================================================
/home/paslechoix/Lorem.txt

bigram sequence is : Apple => bigram seq of apple is : ap pp pl le

File contain some text. As given in RHS (Righ hand side). 
A bigram is pair of successive tokens in some sequence. Please build bigrams from the sequences of words in each sentence, 
and then try to find the most frequently occuring ones. 

val file = sc.textFile("96_sce/prob_68/prob_68.txt").map(rec=>rec.replaceAll(",","").split('.')).map(rec=>rec.map(r=>r.split(" ").filter(rec=>rec!="")))
file.flatMap(rec=>rec).map(rec=>{
var l= List[(String,Int)]()
for(i<-0 to rec.size-1)
{
if(i!=rec.size-1)
{l=l:+((rec(i)+","+rec(i+1)),1)}
else
{l=l:+(rec(i),1)}
}
l
}).first

//to append to list use list:+element
//to prepend to list use list+:element

==========================================================================
Problem Scenario 69(only words) & 70(word count): Write down a Spark Application using Scala, 
==========================================================================
In which it read a file "Content.txt" (On hdfs) with following content. 
And filter out the word which is less than 2 characters and ignore all empty lines. 
Once doen store the filtered data in a directory called "problem84" (On hdfs) 

Content.txt 
Hello this is HadoopExam.com 
This is QuickTechie.com 
Apache Spark Training 
This is Spark Learning Session 
Spark is faster than MapReduce

 val file = sc.textFile("96_sce/prob_69/prob_69.txt").filter(rec=>rec!="" && rec!=" ").flatMap(rec=>rec.split(" ").map(r=>(r.replaceAll("\\.|,|:|;",""),1)))
 file: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[576] at reduceByKey at <console>:30

scala> file.collect
res301: Array[(String, Int)] = Array((QuickTechiecom,1), (this,1), (is,4), (Hello,1), (Apache,1), (MapReduce,1), (Session,1), (This,2), (Training,1), (Learning,1), (Spark,3), (faster,1), (than,1), (HadoopExamcom,1))

===================================================================================
Problem Scenario 71 . 
================================================================================
Write down a Spark script using Python, 
In which it read a file "Content.txt" (On hdfs) with following content. 
After that split each row as (key, value), where key is first word in line and entire line as value. 
Filter out the empty lines. 
And save this key value in "problem86" as Sequence file(On hdfs) 
Part 2 : Save as sequence file , where key as null and entire line as value. Read back the stored sequence files. 

val file = sc.textFile("96_sce/prob_69/prob_69.txt").filter(rec=>rec!="" && rec!=" ").keyBy(rec=>rec.split(" ")(0))
file: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[587] at keyBy at <console>:30
part1
file.saveAsSequenceFile("96_sce/prob_71/part1_seq")

import org.apache.hadoop.io._
part2
file1.map(rec=>(Null,rec._2)).saveAsSequenceFile("96_sce/prob_71/op_seq")

===================================================================================
Problem Scenario 72 : You have been given a table named "employee2" with following detail. 
===================================================================================
first_name string 
last_name string 
Write a spark script in scala which read this table and print all the rows and individual column values. 

for mysql table: export it to hdfs using sqoop:
spark2/employee2

sqoop import \
--connect jdbc:mysql://quickstart.cloudera:3306/practice \
--username root \
--password cloudera \
--table employee2 \
--target-dir 96_sce/prob72/data \
--m 1

case class Employee(fname:String,lname:String)
val emp = sc.textFile("96_sce/prob72/data").map(rec=>employee(rec.split(",")(0),rec.split(",")(1)))

emp.registerTempTable("emp_tbl")

example.map(rec=>rec(0)+"--"+rec(1)).collect.foreach(println)
example.map(rec=>rec.getAs[String]("fname")+"--"+rec.getAs[String]("lname")).collect.foreach(println)

==================================================================================
pakages to import

import org.apache.spark.sql.SQLContext.implicit

==================================================================================
Problem Scenario 73 : You have been given data in json format as below. 

{"first_name":"Ankit","last_name":"jain"}
{"first_name":"Amir","last_name":"Khan"}
{"first_name":"Rajesh", "last_name":"Khanna"}
{"first_name":"Priynka", "last_name":"Chopra"}
{"first_name":"Kareena", "last_name":"Kapoor"}
{"first_name":"Lokesh","last_name":"Yadav"}

val src = sqlContext.read.json("96_sce/prob_73/prob_73.json")
src.registerTempTable("src_tbl")
val src_result = sqlContext.sql("select * from src_tbl")
src_result.write.json("96_sce/prob_73/output")

=================================================================================
Problem Scenario 74 : You have been given MySQL DB with following details. 

User=retail_dba 
password=cloudera 
database=retail_db 
table=retail_db.orders 
Table=retail_db.order_items 
jdbc URL = jdbc:mysql://quickstart:3306/retail_db 
Columns of order table : (order_id , order_date , order_customer_id, order_status) 

Columns of order_items table : (order_item_id , order_item_order_ 
id , order_item_product_id, order_item_quantity,order_item_subtotal,order_
item_product_price) 
Please accomplish following activities. 

1. Copy "retail_db.orders" and "retail_db.order_items" table to hdfs in respective directory p89_orders and p89_order_items 

sqoop import \
--connect jdbc:mysql://quickstart:3306/retail_db \
--username root \
--password cloudera \
--table orders \
--m 1 \
--target-dir 96_sce/prob_74/orders

sqoop import \
--connect jdbc:mysql://quickstart:3306/retail_db \
--username root \
--password cloudera \
--table order_items \
--m 1 \
--target-dir 96_sce/prob_74/order_items

case class Orders(order_id:Int , order_date:String , order_customer_id:Int, order_status:String)
case class Order_Items(order_item_id:Int, order_item_order_id:Int , order_item_product_id:Int, order_item_quantity:Int,order_item_subtotal:Double,order_item_product_price:Double)

val o = sc.textFile("96_sce/prob_74/orders").map(rec=>{
val temp = rec.split(",")
Orders(temp(0).toInt,temp(1),temp(2).toInt,temp(3))
}).map(rec=>(rec.order_id,rec))

val oi = sc.textFile("96_sce/prob_74/order_items").map(rec=>{
val temp = rec.split(",")
Order_Items(temp(0).toInt,temp(1).toInt,temp(2).toInt,temp(3).toInt,temp(4).toDouble,temp(5).toDouble)
}).map(rec=>(rec.order_item_order_id,rec))

val o_oi_rbk = o.join(oi).map(rec=>(rec._2._1.order_date,rec._2._2.order_item_subtotal)).reduceByKey(_+_).sortByKey() // reducceBYkey
val o_oi_gbk = o.join(oi).map(rec=>(rec._2._1.order_date,rec._2._2.order_item_subtotal)).groupByKey().map(rec=>(rec._1,rec._2.sum)).sortByKey()

val o_oi_df = o.join(oi).map(rec=>(rec._2._1.order_date,rec._2._2.order_item_subtotal)).toDF("order_date","subtot")
o_oi_df.registerTempTable("temp_tbl")

=>filter condition on dataframe

o_oi.filter(col("order_id") === 2828).select("order_id","order_date","order_item_subtotal")
o_oi.filter("order_id = 2828")

=> Calculate total order placed for each date, and produced the output sorted by date. 
 
 val o_oi_total = o_oi.groupBy("order_date").agg(sum("order_item_subtotal").alias("total_amount")).orderBy("order_date","total_amount")
==============================================================================================

Problem Scenario 75 : You have been given MySQL DB with following details. 
user=retail_dba 
password=cloudera 
database=retail_db 
table=retail_db.order_items 
jdbc URL = jdbc:mysql://quickstart:3306/retail_db 

Please accomplish following activities. 
1. Copy "retail_db.order_items" table to hdfs in respective directory P90_order_items. 
2. Do the summation of entire revenue in this table using pyspark. 
val sum1 = o.join(oi).map(rec=>rec._2._2.order_item_subtotal).sum

3. Find the maximum and minimum revenue as well. 
val max = o.join(oi).map(rec=>rec._2._2.order_item_subtotal).reduce((max,rev)=>if(max>rev) max else rev)
val min = o.join(oi).map(rec=>rec._2._2.order_item_subtotal).reduce((min,rev)=>if(min<rev) min else rev)

4. Calculate average revenue 
val sum1 = o.join(oi).map(rec=>rec._2._2.order_item_subtotal).count
val avg = sum1/count1


Columns of order_items table : (order_item_id , order_item_order_id , order_item_product_id, order_item_quantity,order_item_subtotal,order_item_product_price) 

==================================================================================================================================
Problem Scenario 76 : You have been given MySQL DB with following details. 

User=retail_dba 
password=cloudera 
Database=retail_db 
Table=retail_db.orders 
jdbc URL = jdbc:mysql://quickstart:3306/retail_db 
Columns ot order table : (order_id , order_date , order_customer_id,order_status) 
Please accomplish following activities. 

1. Copy "retail_db.orders" table to hdfs in a directory p91_orders. 
case class Orders(order_id:Int , order_date:String , order_customer_id:Int, order_status:String)
val o = sc.textFile("96_sce/prob_74/orders").map(rec=>{
val temp = rec.split(",")
Orders(temp(0).toInt,temp(1),temp(2).toInt,temp(3))
})

2. Once data is copied to hdfs, using scala calculate the number of order for each status.

o.map(rec=>(rec.order_status,1)).reduceByKey(_+_).count
o.map(rec=>(rec.order_status,1)).countByKey().count
o.map(rec=>(rec.order_status,1)).groupByKey().map(rec=>(rec._1,rec._2.size)).count
o.map(rec=>rec.order_status).groupBy(rec=>rec).map(rec=>(rec._1,rec._2.size)).count
o.map(rec=>(rec.order_status,1)).combineByKey((rec=>rec),((acc,rec)=>acc+rec),((acc,rec)=>acc+rec)).count
o.map(rec=>(rec.order_status,1)).aggregateByKey(0)(((acc,rec)=>acc+rec),((acc,rec)=>acc+rec)).count

3. use all the following methods to calculate the number of order for each status. (You need to know all these functions and its behavior for real exam) 
- countByKey() 
- groupByKey() 
- reduceByKey() 
- aggregateByKey() 
- combineByKey() 

=======================================================================================================================================
Problem Scenario 78 : You have been given MySQL DB with following details. 

User=retail_dba 
password=cloudera 
Database=retail_db 
table=retail_db.orders 
table=retail_db.order_items 
jdbc URL = jdbc:mysql://quickstart:3306/retail_db 
Columns of order table : (order_id , order_date , order_customer_id, order_status) 
Columns of order_items table : (order_item_id , order_item_order_id , order_item_product_id, order_item_quantity,order_item_subtotal,order_item_product_price)
Please accomplish following activities. 
1. Copy "retail_db.orders" and "retail_db.order_items" table to hdfs in respective directory p92_orders and p92_order_items .

case class Orders(order_id:Int , order_date:String , order_customer_id:Int, order_status:String)
case class Order_Items(order_item_id:Int, order_item_order_id:Int , order_item_product_id:Int, order_item_quantity:Int,order_item_subtotal:Double,order_item_product_price:Double)

val o = sc.textFile("96_sce/prob_74/orders").map(rec=>{
val temp = rec.split(",")
Orders(temp(0).toInt,temp(1),temp(2).toInt,temp(3))
}).map(rec=>(rec.order_id,(rec.order_date,rec.order_customer_id)))

val oi = sc.textFile("96_sce/prob_74/order_items").map(rec=>{
val temp = rec.split(",")
Order_Items(temp(0).toInt,temp(1).toInt,temp(2).toInt,temp(3).toInt,temp(4).toDouble,temp(5).toDouble)
}).map(rec=>(rec.order_item_order_id,rec.order_item_subtotal))

2. Join these data using order_id in Spark and Python 
	val o_oi = o.join(oi).map(rec=>rec._2)

3. Calculate total revenue perday and per customer 
val o_oi = o.join(oi).map(rec=>rec._2)
o_oi: org.apache.spark.rdd.RDD[((String, Int), Double)] = MapPartitionsRDD[436] at map at <console>:35
o_oi.reduceByKey(_+_).map(rec=>(rec._1,BigDecimal(rec._2).setScale(2,scala.math.BigDecimal.RoundingMode.HALF_DOWN)))//dont forget to round the double value

4. Calculate maximum revenue customer
val o_oi = o.join(oi).map(rec=>rec._2).map(rec=>(rec._1._2,rec._2))
o_oi.reduceByKey((acc,rec)=>if(acc>rec) acc else rec)

============================================================================================================================================
Problem Scenario 79-80 : You have been given MySQL DB of following details. 
User=retail_dba 
password=cloudera 
database=retail_db 
table=retail_db.products 
jdbc URL = jdbc:mysql://quickstart:3306/retail_db 

sqoop import \
--connect jdbc:mysql://quickstart:3306/retail_db \
--username root \
--password cloudera \
--table products \
--target-dir 96_sce/prob_79/data \
--m 1

Columns of products table : (product_id | product_category_id | product_name | product_description | product_price | product_image ) 
case class Products(product_id:Int,product_category_id:String,product_name:String ,product_description:String ,product_price:Double ,product_image:String)

Please accomplish following activities. 
1. Copy "retail_db.products" table to hdfs in a directory p93_products 
2. Filter out all the empty prices 
val prod = sc.textFile("96_sce/prob_79/data").filter(rec=>rec.split(",")(4)!="" && rec.split(",")(4).toDouble > 0.0).map(rec=>{
val temp = rec.split(",")
Products(temp(0).toInt,temp(1),temp(2),temp(3),temp(4).toDouble,temp(5))
})

3. Sort all the products based on price in both ascending as well as descending order. 
prod.sortBy(rec=>rec.product_price)
prod.sortBy(rec=> -rec.product_price)

4. Sort all the products based on price as well as product_id in descending order.
prod.sortBy(rec=>(rec.product_price,rec.product_id))
prod.sortBy(rec=>(-rec.product_price,false,-rec.product_id,false))

5. use the below functions to do data ordering or ranking and fetch top 10 elements 
top() 
prod.map(rec=>(rec.product_price,rec.product_id,rec.product_name)).top(10) // order record in descending order and top 10

takeordered() //order product in acsending order 
prod.map(rec=>(rec.product_price,rec.product_id,rec.product_name)).takeOrdered(10)

sortByKey() 

======
Please accomplish following activities. 
1. Copy "retail_db.products" table to hdfs in a directory p93_products 
2. Now sort the products data sorted by product price per category, use product_category_id colunm to group by category 

val prod = sc.textFile("96_sce/prob_79/data").filter(rec=>rec.split(",")(4)!="" && rec.split(",")(4).toDouble > 0.0).map(rec=>{
val temp = rec.split(",")
Products(temp(0).toInt,temp(1),temp(2),temp(3),temp(4).toDouble,temp(5))
}).map(rec=>(rec.product_category_id,rec.product_price)).groupByKey().map(rec=>(rec._1,rec._2.toList.sorted.reverse)).sortByKey()

 prod.map(rec=>(rec._1,rec._2.mkString(","))).take(3).foreach(println)

 =======================================================================================
Problem Scenario 81 : You have been given MySQL DB with following details. 
You have been given following product.csv file 
product.csv (Create this file in hdfs) 
productID,productCode,name,quantity,price 
1001,PEN,pen Red,5000,1.23
1002,PEN,pen Blue,8000,1.25 
1003,PEN,pen Black,8000,1.25
1004,PEC,Pencil 2B,10000,0.48
1005,PEC,Pencil 2H,8000,0.49
1004,PEC,Pencil HB,0,9999.99

1.Create a Hive ORC table using SparkSql 

 sqlContext.sql("CREATE TABLE Products_hive(productID String,productCode String,name String,quantity bigint,price Double)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'")

sqlContext.sql("load data inpath '96_sce/prob_81' into table Products_hive")


 sqlContext.sql("CREATE TABLE Products_orc(productID String,productCode String,name String,quantity bigint,price Double)\nSTORED AS ORC")

 sqlContext.sql("Insert into Products_orc select * from Products_hive")

 sqlContext.sql("select * from Products_orc limit 5").show()

2. Load this data in Hive table. 

3. Create a Hive parquet table using SparkSQL and load data in it. 


=============================================================================
using RDD

case class Products(product_id:Int,product_code:String,product_name:String,product_qty:Long,product_price:Double)
val data = sc.textFile("96_sce/prob_81").mapPartitionsWithIndex((index,rec)=> {if(index==0) rec.drop(1) else rec}).map(rec =>{val temp = rec.split(",")
Products(temp(0).toInt,temp(1),temp(2),temp(3).toLong,temp(4).toDouble)
}).toDF

data.write.mode("overwrite").format("orc").saveAsTable("rdd_orc")

data.write.mode("overwrite").format("parquet").saveAsTable("rdd_parquet")


=====================================================================================
Problem Scenario 82 : You have been given table in Hive with following structure (Which you have created in previous exercise). 
productid int 
code string 
name string 
quantity int 
price float 
using SparkSQL accomplish following activities. 
1. Select all the products name and quantity having quantity <=2000 
sqlContext.sql("select product_name,product_qty from rdd_parquet where product_qty<=2000").show

2. Select name and price of the product having code as 'PEN' 
sqlContext.sql("select product_name,product_price from rdd_parquet where upper(product_code)='PEN'").show

3. Select all the products, which name starts with PENCIL 
sqlContext.sql("select * from rdd_parquet where upper(product_name) like 'PENCIL%'").show

4. Select all products which "name" begins with 'P', followed by any two characters, followed by space, followed by zero or more characters 
sqlContext.sql("select * from rdd_parquet where upper(product_name) like 'P__ %'").show

% - zero or more characters
_ single character
[] sigle character within specific range [a-z]
[^] single character not within specific range [^a-d] not within a-d
===========================================================================================
Problem Scenario 83 : In Continuation of previous question, please accomplish following activities. 
1. Select all the records with quantity >= 5000 and name starts with 'Pen' 
sqlContext.sql("select * from rdd_parquet where product_qty>=5000 and product_name like'Pen%'").show

2. Select all the records with quantity >= 5000, price is less than 1.24 and name starts with 'Pen' 
sqlContext.sql("select * from rdd_parquet where product_qty >= 5000 AND product_price < 1.24 AND product_name like 'Pen%'").show

3. Select all the records witch does not have quantity >= 5000 and name does not starts with 'Pen' 
sqlContext.sql("select * from rdd_parquet where product_qty < 5000 and product_name not like 'Pen%'").show
sqlContext.sql("select * from rdd_parquet where NOT(product_qty >=5000 AND product_name like 'Pen%')").show

4. Select all the products which name is 'Pen Red', 'Pen Black' 
sqlContext.sql("select * from rdd_parquet where product_name='pen Red' OR product_name='pen Black'").show
sqlContext.sql("select * from rdd_parquet where product_name IN ('pen Red','pen Black')")

5. select all the products WhiCh has price BETWEEN 1.0 AND 2.0 AND quantity BETWEEN 1000 AND 2000. 
sqlContext.sql("select * from rdd_parquet where product_price IN (1.0,2.0) AND product_qty IN (1000,2000)").show
sqlContext.sql("select * from rdd_parquet where (product_price BETWEEN 1.0 AND 2.0) AND (product_qty BETWEEN 5000 AND 8000)").show 
//A BETWEEN B A,B inclusive

==================================================================================================================
Problem Scenario 84 : In Continuation of previous question, please accomplish following activities. 
1. Select all the products which has product code as null 
sqlContext.sql("select * from rdd_parquet where product_code is null").show

2. Select all the products, whose name starts with Pen and results should be order by Price descending order. 
sqlContext.sql("select * from rdd_parquet where product_name like 'Pen%' order by product_price desc").show

3. Select all the products, whose name starts with Pen and results should be order by Price descending order and quantity ascending order. 
sqlContext.sql("select * from rdd_parquet where product_name like 'Pen%' order by product_price desc,product_qty asc").show

4. Select top 2 products by price 
sqlContext.sql("select * from rdd_parquet order by product_price limit 2")

------------------------------------------------------------------------------------------------------------------

Problem Scenario 85 : In Continuation of previous question, please accomplish following activities. 
1. Select all the columns from product table with output header as below. 
productiD AS ID 
code AS Code 
name AS Description 
price AS ‘unit Price' 

sqlContext.sql("select product_id as ID,product_code as Code,product_name as Description,product_price as unit_price from rdd_parquet").show

2. Select code and name both separated by ' - ' and header name should be 'product Description'. 
sqlContext.sql("select concat(product_code,'-',product_name) as product_Description from rdd_parquet").show

3. Select all distinct prices. 
sqlContext.sql("select distinct product_price from rdd_parquet").show

4. Select distinct price and name combination. 
sqlContext.sql("select distinct product_price,product_name from rdd_parquet").show

5. Select all price data sorted by both code and productlD combination.
sqlContext.sql("select product_price from rdd_parquet order by product_code,product_id").show

6. count number of products. 
sqlContext.sql("select count(product_id) from rdd_parquet").show

7. Count number of products for each code.
sqlContext.sql("select product_code,count(product_id) from rdd_parquet group by product_code").show

=================================================================================================================
Problem Scenario 86 : In Continuation ot previous question, please accomplish following activities. 
1.Select Maximum, minimum, average , Standard Deviation, and total quantity.
sqlContext.sql("select max(product_price),min(product_price),avg(product_price),stddev(product_price),sum(product_qty) from rdd_parquet").show 

2. Select minimum and maximum price for each product code. 
sqlContext.sql("select product_code,min(product_price) as min_price,max(product_price) as max_price from rdd_parquet group by product_code").show

3. Select Maximum, minimum, average , Standard Deviation, and total quantity for each product code, hwoever make sure Average and Standard deviation will have maximum two decimal values. 
sqlContext.sql("select max(product_price) as max_price,min(product_price) as min_price,round(avg(product_price),2) as avg_price,round(stddev(product_price),2) as stddev_price,sum(product_qty) as total_products from rdd_parquet group by product_code").show

sqlContext.sql("select max(product_price) as max_price,min(product_price) as min_price,CAST(avg(product_price) AS DECIMAL(10,2)) as avg_price,CAST(stddev(product_price) AS DECIMAL(10,2)) as stddev_price,sum(product_qty) as total_products from rdd_parquet group by product_code").show

4. Select all the product code and average price only where product count is more than or equal to 3. 
sqlContext.sql("select product_code,cast(avg(product_price) as DECIMAL(100,2)) as avg_price from rdd_parquet group by product_code having sum(product_qty)>=3 ").show
//use round(rec,2) on safer side

5. Select maximum, minimum , average and total of all the products for each code. Also produce the same across all the products. 
for all products 
sqlContext.sql("select max(product_price) as max_price,min(product_price) as min_price,round(avg(product_price),2) as avg_price,sum(product_qty) as total_products from rdd_parquet").show

for each product
sqlContext.sql("select product_code,max(product_price) as max_price,min(product_price) as min_price,round(avg(product_price),2) as avg_price,sum(product_qty) as total_products from rdd_parquet group by product_code").show
table : rdd_parquet

-------------------------
notes: 

spark-shell --packages com.databricks:spark-avro_2.10:2.0.1

import com.databricks.spark.avro._


Comression

spark.sql.parquet.compression.codec => gzip-default => snappy,lzo,uncompressed
spark.sql.avro.compression.codec

//not available for json,orc

he cat | head of the file.

=========================================
-----------------------
exercise 1
------------------------

1. Import department table in avro format,using one mapper ,snappy compression
/user/cloudera/ce_1/prob1_dept

sqoop import \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username root \
--password cloudera \
--table departments \
--target-dir /user/cloudera/ce_1/prob1_dept \
--as-avrodatafile \
--m 1 \
--compress \
--compression-codec org.apache.hadoop.io.compress.SnappyCodec 

2. Import categories table in parquet format using two mappers,gzip compression
/user/cloudera/ce_1/prob2_cat 

sqoop import \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username root \
--password cloudera \
--table categories \
--target-dir /user/cloudera/ce_1/prob2_cat \
--as-parquetfile \
--m 2 \
--compress \
--compression-codec org.apache.hadoop.io.compress.GzipCodec 



3. load department and categories in dataframe

import com.databricks.spark.avro._
val dept = sqlContext.read.avro("/user/cloudera/ce_1/prob1_dept");
val cat = sqlContext.read.parquet("/user/cloudera/ce_1/prob2_cat");

4. filter categories name which does not contains 'Golf'

cat.registerTempTable("cat_tbl");
val cat_filter = sqlContext.sql("select * from cat_tbl where category_name not like '%Golf%'")

5. find number of categories for each department. output department_id,department_name,count_cate
dept_cat: org.apache.spark.sql.DataFrame = [department_id: int, department_name: string, category_id: int, category_department_id: int, category_name: string]

val dept_cat = dept.join(cat_filter,dept("department_id")===cat_filter("category_department_id")).groupBy("department_id","department_name").agg(count("category_id").alias("count_cate")).orderBy("department_id")

6. save above result in text file(csv) with paruqet compression at the location /user/cloudera/ce_1/text_snappy

dept_cat.map(rec=>rec(0).toString.toInt+","+rec(1).toString+","+rec(2).toString.toInt).saveAsTextFile("/user/cloudera/ce_1/text_snappy",classOf[org.apache.hadoop.io.compress.SnappyCodec])

7. save above file in sequence format at the location /user/cloudera/ce_1/seq_gzip

dept_cat.map(rec=>(rec(0).toString.toInt,rec(1).toString+","+rec(2).toString.toInt)).saveAsSequenceFile("/user/cloudera/ce_1/seq_gzip",classOf[org.apache.hadoop.io.IntWritable],classOf[org.apache.hadoop.io.Text],classOf[org.apache.hadoop.io.compress.GzipCodec])

8. save result 5 in avro file format with snappy compression /user/cloudera/ce_1/avro_snappy

import com.databricks.spark.avro
sqlContext.setConf("spark.sql.avro.compression.codec","snappy")
dept_cat.write.avro("/user/cloudera/ce_1/avro_snappy")


9. export /user/cloudera/ce_1/text_snappy into mysql table custom_db.text_snappy_tbl

create table text_snappy_tbl(department_name varchar(100),department_id Int,count_cate long)

10.
sqlContext.setConf("spark.sql.parquet.compression.codec","snappy")
dept_cat.write.parquet("/user/cloudera/ce_1/parquet_snappy")

sqoop export \
--connect jdbc:mysql://quickstart.cloudera:3306/custom_db \
--username root \
--password cloudera \
--table text_snappy_tbl \
--columns department_id,department_name,count_cate \
--export-dir /user/cloudera/ce_1/avro_snappy

sqoop import \
--connect jdbc:mysql://quickstart.cloudera:3306/custom_db \
--username root \
--password cloudera \
--table text_snappy_tbl \
--target-dir /user/cloudera/ce_1/parquet_snappy \
--as-parquetfile \
--compress \
--compression-codec org.apache.hadoop.io.compress.SnappyCodec \
--m 1

sqoop export \
--connect jdbc:mysql://quickstart.cloudera:3306/custom_db \
--username root \
--password cloudera \
--table text_snappy_tbl \
--export-dir /user/cloudera/ce_1/parquet_snappy




//textFiie export
--input-fields-terminated-by '|'

------------------------
exercise 2
------------------------
